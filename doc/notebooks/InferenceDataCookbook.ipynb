{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Data Cookbook\n",
    "`InferenceData` is the central data format for ArviZ. `InferenceData` itself is just a container that maintains references to one or more `xarray.Dataset`. Below are various ways to generate an `InferenceData` object. See [here](XarrayforArviZ.ipynb) for more on xarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From 1d numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference data with groups:\n",
      "\t> posterior\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:  (chain: 1, draw: 100)\n",
       "Coordinates:\n",
       "  * chain    (chain) int64 0\n",
       "  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 9 ... 90 91 92 93 94 95 96 97 98 99\n",
       "Data variables:\n",
       "    x        (chain, draw) float64 0.293 -0.5543 -0.6883 ... -0.1959 0.02409\n",
       "Attributes:\n",
       "    created_at:  2018-12-08T16:18:45.917214"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = 100\n",
    "dataset = az.convert_to_inference_data(np.random.randn(size))\n",
    "print(dataset)\n",
    "dataset.posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From nd numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference data with groups:\n",
      "\t> posterior\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:  (chain: 1, draw: 2, x_dim_0: 3, x_dim_1: 4, x_dim_2: 5)\n",
       "Coordinates:\n",
       "  * chain    (chain) int64 0\n",
       "  * draw     (draw) int64 0 1\n",
       "  * x_dim_0  (x_dim_0) int64 0 1 2\n",
       "  * x_dim_1  (x_dim_1) int64 0 1 2 3\n",
       "  * x_dim_2  (x_dim_2) int64 0 1 2 3 4\n",
       "Data variables:\n",
       "    x        (chain, draw, x_dim_0, x_dim_1, x_dim_2) float64 0.007123 ... 0.2306\n",
       "Attributes:\n",
       "    created_at:  2018-12-08T16:18:45.933243"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (1, 2, 3, 4, 5)\n",
    "dataset = az.convert_to_inference_data(np.random.randn(*shape))\n",
    "print(dataset)\n",
    "dataset.posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference data with groups:\n",
      "\t> posterior\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:  (b_dim_0: 10, c_dim_0: 3, c_dim_1: 4, chain: 1, draw: 100)\n",
       "Coordinates:\n",
       "  * chain    (chain) int64 0\n",
       "  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 9 ... 90 91 92 93 94 95 96 97 98 99\n",
       "  * b_dim_0  (b_dim_0) int64 0 1 2 3 4 5 6 7 8 9\n",
       "  * c_dim_0  (c_dim_0) int64 0 1 2\n",
       "  * c_dim_1  (c_dim_1) int64 0 1 2 3\n",
       "Data variables:\n",
       "    a        (chain, draw) float64 -1.483 -1.244 -0.2315 ... -1.501 0.4877\n",
       "    b        (chain, draw, b_dim_0) float64 2.497 1.039 1.755 ... -2.279 0.3407\n",
       "    c        (chain, draw, c_dim_0, c_dim_1) float64 -0.4251 0.0699 ... 1.298\n",
       "Attributes:\n",
       "    created_at:  2018-12-08T16:18:45.954741"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datadict = {\n",
    "    'a': np.random.randn(100),\n",
    "    'b': np.random.randn(1, 100, 10),\n",
    "    'c': np.random.randn(1, 100, 3, 4),\n",
    "}\n",
    "dataset = az.convert_to_inference_data(datadict)\n",
    "print(dataset)\n",
    "dataset.posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From dictionary with coords and dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference data with groups:\n",
      "\t> posterior\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:  (b1: 10, c1: 3, c2: 4, chain: 1, draw: 100)\n",
       "Coordinates:\n",
       "  * chain    (chain) int64 0\n",
       "  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 9 ... 90 91 92 93 94 95 96 97 98 99\n",
       "  * b1       (b1) int64 0 1 2 3 4 5 6 7 8 9\n",
       "  * c1       (c1) int64 0 1 2\n",
       "  * c2       (c2) int64 0 1 2 3\n",
       "Data variables:\n",
       "    a        (chain, draw) float64 -2.048 -0.3983 1.1 ... -0.2418 0.3686 -0.1518\n",
       "    b        (chain, draw, b1) float64 -0.02054 -3.143 ... -0.9643 -0.4444\n",
       "    c        (chain, draw, c1, c2) float64 2.43 -0.439 ... -0.3713 -0.5101\n",
       "Attributes:\n",
       "    created_at:  2018-12-08T16:18:45.992123"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datadict = {\n",
    "    'a': np.random.randn(100),\n",
    "    'b': np.random.randn(1, 100, 10),\n",
    "    'c': np.random.randn(1, 100, 3, 4),\n",
    "}\n",
    "coords = {'c1' : np.arange(3), 'c2' : np.arange(4), 'b1' : np.arange(10)}\n",
    "dims = {'b' : ['b1'], 'c' : ['c1', 'c2']}\n",
    "\n",
    "dataset = az.convert_to_inference_data(datadict, coords=coords, dims=dims)\n",
    "print(dataset)\n",
    "dataset.posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From pymc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 4 jobs)\n",
      "NUTS: [theta_tilde, tau, mu]\n",
      "Sampling 2 chains: 100%|██████████| 2000/2000 [00:01<00:00, 1407.23draws/s]\n",
      "There was 1 divergence after tuning. Increase `target_accept` or reparameterize.\n",
      "100%|██████████| 500/500 [00:00<00:00, 1951.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Inference data with groups:\n",
       "\t> posterior\n",
       "\t> sample_stats\n",
       "\t> posterior_predictive\n",
       "\t> prior\n",
       "\t> observed_data"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymc3 as pm\n",
    "draws = 500\n",
    "chains = 2\n",
    "\n",
    "eight_school_data = {'J': 8,\n",
    "                     'y': np.array([28., 8., -3., 7., -1., 1., 18., 12.]),\n",
    "                     'sigma': np.array([15., 10., 16., 11., 9., 11., 10., 18.])\n",
    "                    }\n",
    "\n",
    "with pm.Model() as model:\n",
    "    mu = pm.Normal('mu', mu=0, sd=5)\n",
    "    tau = pm.HalfCauchy('tau', beta=5)\n",
    "    theta_tilde = pm.Normal('theta_tilde', mu=0, sd=1, shape=eight_school_data['J'])\n",
    "    theta = pm.Deterministic('theta', mu + tau * theta_tilde)\n",
    "    pm.Normal('obs', mu=theta, sd=eight_school_data['sigma'], observed=eight_school_data['y'])\n",
    "    \n",
    "    trace = pm.sample(draws, chains=chains)\n",
    "    prior = pm.sample_prior_predictive()\n",
    "    posterior_predictive = pm.sample_posterior_predictive(trace, 500, model)\n",
    "    \n",
    "    data = az.from_pymc3(\n",
    "            trace=trace,\n",
    "            prior=prior,\n",
    "            posterior_predictive=posterior_predictive,\n",
    "            coords={'school': np.arange(eight_school_data['J'])},\n",
    "            dims={'theta': ['school'], 'theta_tilde': ['school']},\n",
    "        )\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From pystan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_e7aeb8b836685a923269e6171e7377cd NOW.\n",
      "/home/tbayes/miniconda3/envs/arviz3.6/lib/python3.6/site-packages/Cython/Compiler/Main.py:367: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /tmp/tmpehtg_iwy/stanfit4anon_model_e7aeb8b836685a923269e6171e7377cd_1721201981547910013.pyx\n",
      "  tree = Parsing.p_module(s, pxd, full_module_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In file included from /home/tbayes/miniconda3/envs/arviz3.6/lib/python3.6/site-packages/numpy/core/include/numpy/ndarraytypes.h:1821:0,\n",
      "                 from /home/tbayes/miniconda3/envs/arviz3.6/lib/python3.6/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,\n",
      "                 from /home/tbayes/miniconda3/envs/arviz3.6/lib/python3.6/site-packages/numpy/core/include/numpy/arrayobject.h:4,\n",
      "                 from /tmp/tmpehtg_iwy/stanfit4anon_model_e7aeb8b836685a923269e6171e7377cd_1721201981547910013.cpp:688:\n",
      "/home/tbayes/miniconda3/envs/arviz3.6/lib/python3.6/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning \"Using deprecated NumPy API, disable it by \" \"#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]\n",
      " #warning \"Using deprecated NumPy API, disable it by \" \\\n",
      "  ^\n",
      "In file included from /tmp/tmpehtg_iwy/stanfit4anon_model_e7aeb8b836685a923269e6171e7377cd_1721201981547910013.cpp:692:0:\n",
      "/home/tbayes/miniconda3/envs/arviz3.6/lib/python3.6/site-packages/pystan/stan_fit.hpp: In function ‘int pystan::{anonymous}::command(pystan::StanArgs&, Model&, pystan::StanHolder&, const std::vector<long unsigned int>&, const std::vector<std::__cxx11::basic_string<char> >&, RNG_t&)’:\n",
      "/home/tbayes/miniconda3/envs/arviz3.6/lib/python3.6/site-packages/pystan/stan_fit.hpp:830:12: warning: ‘template<class> class std::auto_ptr’ is deprecated [-Wdeprecated-declarations]\n",
      "       std::auto_ptr<stan::io::var_context> init_context_ptr;\n",
      "            ^\n",
      "In file included from /usr/include/c++/5/bits/locale_conv.h:41:0,\n",
      "                 from /usr/include/c++/5/locale:43,\n",
      "                 from /usr/include/c++/5/iomanip:43,\n",
      "                 from /home/tbayes/miniconda3/envs/arviz3.6/lib/python3.6/site-packages/pystan/stan_fit.hpp:5,\n",
      "                 from /tmp/tmpehtg_iwy/stanfit4anon_model_e7aeb8b836685a923269e6171e7377cd_1721201981547910013.cpp:692:\n",
      "/usr/include/c++/5/bits/unique_ptr.h:49:28: note: declared here\n",
      "   template<typename> class auto_ptr;\n",
      "                            ^\n",
      "/tmp/tmpehtg_iwy/stanfit4anon_model_e7aeb8b836685a923269e6171e7377cd_1721201981547910013.cpp: In function ‘PyObject* __pyx_pf_71stanfit4anon_model_e7aeb8b836685a923269e6171e7377cd_1721201981547910013_2_call_sampler(PyObject*, PyObject*, PyObject*, PyObject*)’:\n",
      "/tmp/tmpehtg_iwy/stanfit4anon_model_e7aeb8b836685a923269e6171e7377cd_1721201981547910013.cpp:9242:30: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
      "     __pyx_t_12 = ((__pyx_t_9 != __pyx_v_fitptr->param_names_oi().size()) != 0);\n",
      "                              ^\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:Rhat for parameter mu is 1.1939833216977542!\n",
      "WARNING:pystan:Rhat for parameter tau is 1.1755437966827746!\n",
      "WARNING:pystan:Rhat for parameter theta[1] is 1.1312100863716745!\n",
      "WARNING:pystan:Rhat for parameter theta[2] is 1.1005746998099943!\n",
      "WARNING:pystan:Rhat for parameter log_lik[1] is 1.1646655752781883!\n",
      "WARNING:pystan:Rhat for parameter log_lik[2] is 1.1564767449269702!\n",
      "WARNING:pystan:Rhat above 1.1 or below 0.9 indicates that the chains very likely have not mixed\n",
      "WARNING:pystan:72 of 1000 iterations ended with a divergence (7.2%).\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.8 to remove the divergences.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Inference data with groups:\n",
       "\t> posterior\n",
       "\t> sample_stats\n",
       "\t> posterior_predictive\n",
       "\t> observed_data"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pystan\n",
    "\n",
    "schools_code = '''\n",
    "        data {\n",
    "            int<lower=0> J;\n",
    "            real y[J];\n",
    "            real<lower=0> sigma[J];\n",
    "        }\n",
    "\n",
    "        parameters {\n",
    "            real mu;\n",
    "            real<lower=0> tau;\n",
    "            real theta_tilde[J];\n",
    "        }\n",
    "\n",
    "        transformed parameters {\n",
    "            real theta[J];\n",
    "            for (j in 1:J)\n",
    "                theta[j] = mu + tau * theta_tilde[j];\n",
    "        }\n",
    "\n",
    "        model {\n",
    "            mu ~ normal(0, 5);\n",
    "            tau ~ cauchy(0, 5);\n",
    "            theta_tilde ~ normal(0, 1);\n",
    "            y ~ normal(theta, sigma);\n",
    "        }\n",
    "\n",
    "        generated quantities {\n",
    "            vector[J] log_lik;\n",
    "            vector[J] y_hat;\n",
    "            for (j in 1:J) {\n",
    "                log_lik[j] = normal_lpdf(y[j] | theta[j], sigma[j]);\n",
    "                y_hat[j] = normal_rng(theta[j], sigma[j]);\n",
    "            }\n",
    "        }\n",
    "    '''\n",
    "stan_model = pystan.StanModel(model_code=schools_code)\n",
    "fit = stan_model.sampling(data=eight_school_data,\n",
    "                          iter=draws,\n",
    "                          warmup=0,\n",
    "                          chains=chains)\n",
    "\n",
    "data = az.from_pystan(posterior=fit,\n",
    "                      posterior_predictive='y_hat',\n",
    "                      observed_data=['y'],\n",
    "                      log_likelihood='log_lik',\n",
    "                      coords={'school': np.arange(eight_school_data['J'])},\n",
    "                      dims={'theta': ['school'],\n",
    "                             'y': ['school'],\n",
    "                             'log_lik': ['school'],\n",
    "                             'y_hat': ['school'],\n",
    "                             'theta_tilde': ['school']\n",
    "                            }\n",
    "                     )\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From pyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyro.infer.mcmc.mcmc:Starting MCMC using kernel - NUTS ...\n",
      "INFO:pyro.infer.mcmc.mcmc:Iteration: 50 [SAMPLE]\n",
      "INFO:pyro.infer.mcmc.mcmc:Step size: 0.125390 \t Acceptance rate: 0.860000\n",
      "INFO:pyro.infer.mcmc.mcmc:Iteration: 100 [SAMPLE]\n",
      "INFO:pyro.infer.mcmc.mcmc:Step size: 0.789698 \t Acceptance rate: 0.910000\n",
      "INFO:pyro.infer.mcmc.mcmc:Iteration: 150 [SAMPLE]\n",
      "INFO:pyro.infer.mcmc.mcmc:Step size: 0.282786 \t Acceptance rate: 0.920000\n",
      "INFO:pyro.infer.mcmc.mcmc:Iteration: 200 [SAMPLE]\n",
      "INFO:pyro.infer.mcmc.mcmc:Step size: 0.105557 \t Acceptance rate: 0.930000\n",
      "INFO:pyro.infer.mcmc.mcmc:Iteration: 250 [SAMPLE]\n",
      "INFO:pyro.infer.mcmc.mcmc:Step size: 0.527265 \t Acceptance rate: 0.944000\n",
      "INFO:pyro.infer.mcmc.mcmc:Iteration: 300 [SAMPLE]\n",
      "INFO:pyro.infer.mcmc.mcmc:Step size: 0.200404 \t Acceptance rate: 0.940000\n",
      "INFO:pyro.infer.mcmc.mcmc:Iteration: 350 [SAMPLE]\n",
      "INFO:pyro.infer.mcmc.mcmc:Step size: 0.286618 \t Acceptance rate: 0.945714\n",
      "INFO:pyro.infer.mcmc.mcmc:Iteration: 400 [SAMPLE]\n",
      "INFO:pyro.infer.mcmc.mcmc:Step size: 0.323607 \t Acceptance rate: 0.945000\n",
      "INFO:pyro.infer.mcmc.mcmc:Iteration: 450 [SAMPLE]\n",
      "INFO:pyro.infer.mcmc.mcmc:Step size: 0.600277 \t Acceptance rate: 0.948889\n",
      "INFO:pyro.infer.mcmc.mcmc:Iteration: 500 [SAMPLE]\n",
      "INFO:pyro.infer.mcmc.mcmc:Step size: 0.265936 \t Acceptance rate: 0.950000\n",
      "INFO:pyro.infer.mcmc.mcmc:Iteration: 550 [SAMPLE]\n",
      "INFO:pyro.infer.mcmc.mcmc:Step size: 0.423345 \t Acceptance rate: 0.945455\n",
      "INFO:pyro.infer.mcmc.mcmc:Iteration: 600 [SAMPLE]\n",
      "INFO:pyro.infer.mcmc.mcmc:Step size: 0.675079 \t Acceptance rate: 0.948333\n",
      "INFO:pyro.infer.mcmc.mcmc:Iteration: 650 [SAMPLE]\n",
      "INFO:pyro.infer.mcmc.mcmc:Step size: 0.523993 \t Acceptance rate: 0.949231\n",
      "INFO:pyro.infer.mcmc.mcmc:Iteration: 700 [SAMPLE]\n",
      "INFO:pyro.infer.mcmc.mcmc:Step size: 0.372428 \t Acceptance rate: 0.948571\n",
      "INFO:pyro.infer.mcmc.mcmc:Iteration: 750 [SAMPLE]\n",
      "INFO:pyro.infer.mcmc.mcmc:Step size: 0.536313 \t Acceptance rate: 0.950667\n",
      "INFO:pyro.infer.mcmc.mcmc:Iteration: 800 [SAMPLE]\n",
      "INFO:pyro.infer.mcmc.mcmc:Step size: 0.508929 \t Acceptance rate: 0.948750\n",
      "INFO:pyro.infer.mcmc.mcmc:Iteration: 850 [SAMPLE]\n",
      "INFO:pyro.infer.mcmc.mcmc:Step size: 0.551862 \t Acceptance rate: 0.948235\n",
      "INFO:pyro.infer.mcmc.mcmc:Iteration: 900 [SAMPLE]\n",
      "INFO:pyro.infer.mcmc.mcmc:Step size: 0.283344 \t Acceptance rate: 0.948889\n",
      "INFO:pyro.infer.mcmc.mcmc:Iteration: 950 [SAMPLE]\n",
      "INFO:pyro.infer.mcmc.mcmc:Step size: 0.411292 \t Acceptance rate: 0.950526\n",
      "INFO:pyro.infer.mcmc.mcmc:Iteration: 1000 [SAMPLE]\n",
      "INFO:pyro.infer.mcmc.mcmc:Step size: 0.372683 \t Acceptance rate: 0.950000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Inference data with groups:\n",
       "\t> posterior\n",
       "\t> observed_data"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "from pyro.infer.mcmc import MCMC, NUTS\n",
    "\n",
    "pyro.enable_validation(True)\n",
    "pyro.set_rng_seed(0)\n",
    "\n",
    "draws = 1000\n",
    "warmup_steps = 0\n",
    "eight_school_data = {'J' : 8,\n",
    "                     'y' : torch.tensor([28,  8, -3,  7, -1,  1, 18, 12]).type(torch.Tensor),\n",
    "                     'sigma' : torch.tensor([15, 10, 16, 11, 9, 11, 10, 18]).type(torch.Tensor)\n",
    "                    }\n",
    "\n",
    "\n",
    "def model(sigma):\n",
    "    eta = pyro.sample('eta', dist.Normal(torch.zeros(eight_school_data['J']), torch.ones(eight_school_data['J'])))\n",
    "    mu = pyro.sample('mu', dist.Normal(torch.zeros(1), 10 * torch.ones(1)))\n",
    "    tau = pyro.sample('tau', dist.HalfCauchy(scale=25 * torch.ones(1)))\n",
    "\n",
    "    theta = mu + tau * eta\n",
    "\n",
    "    return pyro.sample(\"obs\", dist.Normal(theta, sigma))\n",
    "\n",
    "\n",
    "def conditioned_model(model, sigma, y):\n",
    "    return poutine.condition(model, data={\"obs\": y})(sigma)\n",
    "\n",
    "\n",
    "\n",
    "nuts_kernel = NUTS(conditioned_model, adapt_step_size=True)\n",
    "posterior = MCMC(nuts_kernel,\n",
    "                 num_samples=draws,\n",
    "                 warmup_steps=warmup_steps).run(model, eight_school_data['sigma'], eight_school_data['y'])\n",
    "\n",
    "pyro_data = az.from_pyro(posterior)\n",
    "pyro_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From emcee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Inference data with groups:\n",
       "\t> posterior\n",
       "\t> observed_data"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import emcee\n",
    "\n",
    "eight_school_data = {'J': 8,\n",
    "                     'y': np.array([28., 8., -3., 7., -1., 1., 18., 12.]),\n",
    "                     'sigma': np.array([15., 10., 16., 11., 9., 11., 10., 18.])\n",
    "                    }\n",
    "\n",
    "def log_prior_8school(theta,J):\n",
    "    mu = theta[0]\n",
    "    tau = theta[1]\n",
    "    eta = theta[2:]\n",
    "    # Half-cauchy prior\n",
    "    if tau<0:\n",
    "        return -np.inf\n",
    "    hwhm = 25\n",
    "    prior_tau = -np.log(tau**2+hwhm**2)\n",
    "    prior_mu = -(mu/10)**2  # normal prior, loc=0, scale=10\n",
    "    prior_eta = -np.sum(eta**2)  # normal prior, loc=0, scale=1\n",
    "    return prior_mu + prior_tau + prior_eta\n",
    "    \n",
    "def log_likelihood_8school(theta,y,sigma):\n",
    "    mu = theta[0]\n",
    "    tau = theta[1]\n",
    "    eta = theta[2:]\n",
    "    return -np.sum(((mu + tau * eta - y) / sigma)**2)\n",
    "    \n",
    "def lnprob_8school(theta,J,y,sigma):\n",
    "    prior = log_prior_8school(theta,J)\n",
    "    if prior <= -np.inf:\n",
    "        return -np.inf\n",
    "    like = log_likelihood_8school(theta,y,sigma)\n",
    "    return like+prior\n",
    "\n",
    "nwalkers = 40\n",
    "ndim = eight_school_data['J']+2\n",
    "draws = 1500\n",
    "pos = np.random.normal(size=(nwalkers,ndim))\n",
    "pos[:,1] = np.absolute(pos[:,1])\n",
    "sampler = emcee.EnsembleSampler(nwalkers, \n",
    "                                ndim, \n",
    "                                lnprob_8school, \n",
    "                                args=(eight_school_data['J'], \n",
    "                                      eight_school_data['y'], \n",
    "                                      eight_school_data['sigma']\n",
    "                                     )\n",
    "                               )\n",
    "sampler.run_mcmc(pos, draws)\n",
    "\n",
    "# define variable names, it cannot be inferred from emcee\n",
    "var_names = ['mu','tau']+['eta{}'.format(i) for i in range(eight_school_data['J'])]\n",
    "emcee_data = az.from_emcee(sampler, var_names = var_names)\n",
    "emcee_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From cmdstan\n",
    "See [from_cmdstan](https://arviz-devs.github.io/arviz/generated/arviz.from_cmdstan.html#arviz.from_cmdstan) for details. Cookbook documentation coming soon."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arviz3.6",
   "language": "python",
   "name": "arviz3_6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
