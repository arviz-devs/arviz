@article{GabryVisualizationBayesianworkflow2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1709.01449},
  primaryClass = {stat},
  title = {Visualization in {{Bayesian}} Workflow},
  abstract = {Bayesian data analysis is about more than just computing a posterior distribution, and Bayesian visualization is about more than trace plots of Markov chains. Practical Bayesian data analysis, like all data analysis, is an iterative process of model building, inference, model checking and evaluation, and model expansion. Visualization is helpful in each of these stages of the Bayesian workflow and it is indispensable when drawing inferences from the types of modern, high-dimensional models that are used by applied researchers.},
  journal = {arXiv:1709.01449 [stat]},
  author = {Gabry, Jonah and Simpson, Daniel and Vehtari, Aki and Betancourt, Michael and Gelman, Andrew},
  month = sep,
  year = {2017},
  keywords = {Statistics - Methodology,Statistics - Applications},
  file = {/home/osvaldo/Zotero/storage/KJYIRJWV/Gabry et al. - 2017 - Visualization in Bayesian workflow.pdf;/home/osvaldo/Zotero/storage/985JKW2Q/1709.html},
  annote = {Comment: 17 pages, 11 Figures. Includes supplementary material}
}

@article{VehtariPracticalBayesianModel2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1507.04544},
  title = {Practical {{Bayesian Model Evaluation Using Leave}}-{{One}}-out {{Cross}}-{{Validation}} and {{WAIC}}},
  abstract = {Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We introduce an efficient computation of LOO using Pareto-smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case with weak priors or influential observations. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparing of predictive errors between two models. We implement the computations in an R package called 'loo' and demonstrate using models fit with the Bayesian inference package Stan.},
  journal = {arXiv:1507.04544 [stat]},
  author = {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
  month = jul,
  year = {2015},
  keywords = {Statistics - Computation,Statistics - Methodology}
}

@article{watanabe_widely_2013,
  title = {A {{Widely Applicable Bayesian Information Criterion}}},
  volume = {14},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1208.6338},
  abstract = {A statistical model or a learning machine is called regular if the map taking a parameter
to a probability distribution is one-to-one and if its Fisher information
matrix is always positive definite. If otherwise, it is called singular.
In regular statistical models,
the Bayes free energy, which is defined by the minus logarithm of Bayes marginal likelihood,
can be asymptotically approximated by the
Schwarz Bayes information criterion (BIC), whereas in singular models
such approximation does not hold.

Recently, it was proved that the Bayes free energy of a singular model is
asymptotically given by a generalized formula
using a birational invariant, the real log canonical threshold (RLCT),
instead of half the number of parameters in BIC.
Theoretical values of RLCTs in several statistical models are now being
discovered based on algebraic geometrical methodology.
However, it has been difficult to estimate the Bayes free energy using only training samples,
because an RLCT depends on an unknown true distribution.

In the present paper, we define a widely applicable Bayesian information criterion (WBIC) by
the average log likelihood function
over the posterior distribution with the inverse temperature 1/log n,
where n is the number of training samples. We mathematically prove that
WBIC has the same asymptotic expansion as the Bayes free energy, even if
a statistical model is singular for or  unrealizable by a statistical model.
Since WBIC can be numerically calculated without any information about a true
distribution,
it is a generalized version of BIC onto singular statistical models.},
  journal = {Journal of Machine Learning Research},
  author = {Watanabe, Sumio},
  month = mar,
  year = {2013},
  pages = {867-897},
  file = {/home/osvaldo/Zotero/storage/424KD4K3/Watanabe - 2013 - A Widely Applicable Bayesian Information Criterion.pdf}
}

@article{HoyerxarrayNDlabeled2017,
  title = {Xarray: {{N}}-{{D}} Labeled {{Arrays}} and {{Datasets}} in {{Python}}},
  volume = {5},
  copyright = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are \textcopyright, \textregistered{} or \texttrademark{} of their respective owners. No challenge to any owner's rights is intended or should be inferred.},
  issn = {2049-9647},
  shorttitle = {Xarray},
  doi = {10.5334/jors.148},
  abstract = {xarray is an open source project and Python package that provides a toolkit and data structures for N-dimensional labeled arrays. Our approach combines an application programing interface (API) inspired by pandas with the Common Data Model for self-described scientific data. Key features of the xarray package include label-based indexing and arithmetic, interoperability with the core scientific Python packages (e.g., pandas, NumPy, Matplotlib), out-of-core computation on datasets that don't fit into memory, a wide range of serialization and input/output (I/O) options, and advanced multi-dimensional data manipulation tools such as group-by and resampling. xarray, as a data model and analytics toolkit, has been widely adopted in the geoscience community but is also used more broadly for multi-dimensional data analysis in physics, machine learning and finance.},
  language = {en},
  number = {1},
  journal = {Journal of Open Research Software},
  author = {Hoyer, Stephan and Hamman, Joe},
  month = apr,
  year = {2017},
  keywords = {Python,data analysis,data; data handling,multidimensional,netCDF,pandas},
  file = {/home/osvaldo/Zotero/storage/AGIDDAZZ/Hoyer y Hamman - 2017 - xarray N-D labeled Arrays and Datasets in Python.pdf;/home/osvaldo/Zotero/storage/RQ254VLT/jors.html}
}


@ARTICLE{56302,
author={R. Rew and G. Davis},
journal={IEEE Computer Graphics and Applications},
title={NetCDF: an interface for scientific data access},
year={1990},
volume={10},
number={4},
pages={76-82},
keywords={computer graphics;data structures;network operating systems;NetCDF;scientific data access;data abstraction;multidimensional data;software library;heterogeneous network environment;application software;standard;external data representation;dimensions;variables;attributes;direct access;hyperslab access;Application software;Data visualization;Workstations;Software libraries;Educational institutions;Computer displays;Meteorology;Computer architecture;Information retrieval;Multidimensional systems},
doi={10.1109/38.56302},
ISSN={0272-1716},
month={July},}

@article{brown_1993,
author = {Brown,Stewart A.  and Folk,Mike  and Goucher,Gregory  and Rew,Russ  and Dubois,Paul F. },
title = {Software for Portable Scientific Data Management},
journal = {Computers in Physics},
volume = {7},
number = {3},
pages = {304-308},
year = {1993},
doi = {10.1063/1.4823180},

URL = {
        https://aip.scitation.org/doi/abs/10.1063/1.4823180

},
eprint = {
        https://aip.scitation.org/doi/pdf/10.1063/1.4823180

}

}


@misc{zotero-null-295,
  title = {Probabilistic {{Programming}}},
  howpublished = {http://probabilistic-programming.org/wiki/Home},
  journal = {http://probabilistic-programming.org/wiki/Home},
  author = {{Daniel Roy}},
  year = {2015},
  file = {/home/osvaldo/Zotero/storage/6UZ84C6Q/Home.html}
}


@book{bessiere_bayesian_2013,
  address = {Boca Raton},
  edition = {1 edition},
  title = {Bayesian {{Programming}}},
  isbn = {978-1-4398-8032-6},
  abstract = {Probability as an Alternative to Boolean LogicWhile logic is the mathematical foundation of rational reasoning and the fundamental principle of computing, it is restricted to problems where information is both complete and certain. However, many real-world problems, from financial investments to email filtering, are incomplete or uncertain in nature. Probability theory and Bayesian computing together provide an alternative framework to deal with incomplete and uncertain data.  Decision-Making Tools and Methods for Incomplete and Uncertain DataEmphasizing probability as an alternative to Boolean logic, Bayesian Programming covers new methods to build probabilistic programs for real-world applications. Written by the team who designed and implemented an efficient probabilistic inference engine to interpret Bayesian programs, the book offers many Python examples that are also available on a supplementary website together with an interpreter that allows readers to experiment with this new approach to programming. Principles and Modeling Only requiring a basic foundation in mathematics, the first two parts of the book present a new methodology for building subjective probabilistic models. The authors introduce the principles of Bayesian programming and discuss good practices for probabilistic modeling. Numerous simple examples highlight the application of Bayesian modeling in different fields. Formalism and AlgorithmsThe third part synthesizes existing work on Bayesian inference algorithms since an efficient Bayesian inference engine is needed to automate the probabilistic calculus in Bayesian programs. Many bibliographic references are included for readers who would like more details on the formalism of Bayesian programming, the main probabilistic models, general purpose algorithms for Bayesian inference, and learning problems. FAQsAlong with a glossary, the fourth part contains answers to frequently asked questions. The authors compare Bayesian programming and possibility theories, discuss the computational complexity of Bayesian inference, cover the irreducibility of incompleteness, and address the subjectivist versus objectivist epistemology of probability.  The First Steps toward a Bayesian ComputerA new modeling methodology, new inference algorithms, new programming languages, and new hardware are all needed to create a complete Bayesian computing framework. Focusing on the methodology and algorithms, this book describes the first steps toward reaching that goal. It encourages readers to explore emerging areas, such as bio-inspired computing, and develop new programming languages and hardware architectures.},
  language = {English},
  publisher = {{Chapman and Hall/CRC}},
  author = {Bessiere, Pierre and Mazer, Emmanuel and Ahuactzin, Juan Manuel and Mekhnacha, Kamel},
  month = dec,
  year = {2013}
}


@article{Ghahramani2015,
  title = {Probabilistic Machine Learning and Artificial Intelligence},
  volume = {521},
  copyright = {\textcopyright{} 2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn = {0028-0836},
  doi = {10.1038/nature14541},
  abstract = {How can a machine learn from experience? Probabilistic modelling provides a framework for understanding what learning is, and has therefore emerged as one of the principal theoretical and practical approaches for designing machines that learn from data acquired through experience. The probabilistic framework, which describes how to represent and manipulate uncertainty about models and predictions, has a central role in scientific data analysis, machine learning, robotics, cognitive science and artificial intelligence. This Review provides an introduction to this framework, and discusses some of the state-of-the-art advances in the field, namely, probabilistic programming, Bayesian optimization, data compression and automatic model discovery.},
  language = {en},
  number = {7553},
  journal = {Nature},
  author = {Ghahramani, Zoubin},
  month = may,
  year = {2015},
  keywords = {neuroscience,Computer science,Mathematics and computing},
  pages = {452-459},
  file = {/home/osvaldo/Zotero/storage/9PJBVRVB/nature14541.html}
}


@book{TukeyExploratoryDataAnalysis1977,
  address = {Reading, Mass},
  edition = {1 edition},
  title = {Exploratory {{Data Analysis}}},
  isbn = {978-0-201-07616-5},
  abstract = {The approach in this introductory book is that of informal study of the data. Methods range from plotting picture-drawing techniques to rather elaborate numerical summaries. Several of the methods are the original creations of the author, and all can be carried out either with pencil or aided by hand-held calculator.},
  language = {English},
  publisher = {{Pearson}},
  author = {Tukey, John W.},
  year = {1977}
}


@article{GabryVisualizationBayesianworkflow2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1709.01449},
  primaryClass = {stat},
  title = {Visualization in {{Bayesian}} Workflow},
  abstract = {Bayesian data analysis is about more than just computing a posterior distribution, and Bayesian visualization is about more than trace plots of Markov chains. Practical Bayesian data analysis, like all data analysis, is an iterative process of model building, inference, model checking and evaluation, and model expansion. Visualization is helpful in each of these stages of the Bayesian workflow and it is indispensable when drawing inferences from the types of modern, high-dimensional models that are used by applied researchers.},
  journal = {arXiv:1709.01449 [stat]},
  author = {Gabry, Jonah and Simpson, Daniel and Vehtari, Aki and Betancourt, Michael and Gelman, Andrew},
  month = sep,
  year = {2017},
  keywords = {Statistics - Methodology,Statistics - Applications},
  file = {/home/osvaldo/Zotero/storage/KJYIRJWV/Gabry et al. - 2017 - Visualization in Bayesian workflow.pdf;/home/osvaldo/Zotero/storage/985JKW2Q/1709.html},
  annote = {Comment: 17 pages, 11 Figures. Includes supplementary material}
}


@article{GelmanBayesianFormulationExploratory2003,
  title = {A {{Bayesian Formulation}} of {{Exploratory Data Analysis}} and {{Goodness}}-of-Fit {{Testing}}*},
  volume = {71},
  issn = {1751-5823},
  doi = {10.1111/j.1751-5823.2003.tb00203.x},
  abstract = {Exploratory data analysis (EDA) and Bayesian inference (or, more generally, complex statistical modeling)\textemdash{}which are generally considered as unrelated statistical paradigms\textemdash{}can be particularly effective in combination. In this paper, we present a Bayesian framework for EDA based on posterior predictive checks. We explain how posterior predictive simulations can be used to create reference distributions for EDA graphs, and how this approach resolves some theoretical problems in Bayesian data analysis. We show how the generalization of Bayesian inference to include replicated data yrep and replicated parameters \texttheta{}rep follows a long tradition of generalizations in Bayesian theory. On the theoretical level, we present a predictive Bayesian formulation of goodness-of-fit testing, distinguishing between p-values (posterior probabilities that specified antisymmetric discrepancy measures will exceed 0) and u-values (data summaries with uniform sampling distributions). We explain that p-values, unlike u-values, are Bayesian probability statements in that they condition on observed data. Having reviewed the general theoretical framework, we discuss the implications for statistical graphics and exploratory data analysis, with the goal being to unify exploratory data analysis with more formal statistical methods based on probability models. We interpret various graphical displays as posterior predictive checks and discuss how Bayesian inference can be used to determine reference distributions. The goal of this work is not to downgrade descriptive statistics, or to suggest they be replaced by Bayesian modeling, but rather to suggest how exploratory data analysis fits into the probability-modeling paradigm. We conclude with a discussion of the implications for practical Bayesian inference. In particular, we anticipate that Bayesian software can be generalized to draw simulations of replicated data and parameters from their posterior predictive distribution, and these can in turn be used to calibrate EDA graphs.},
  language = {en},
  number = {2},
  journal = {International Statistical Review},
  author = {Gelman, Andrew},
  month = aug,
  year = {2003},
  keywords = {“p-value”,“u-value”,Bootstrap,Fisher's exact test,Graphics,Graphiques,Mixture model,Model checking,Modéles de mèlange,Multiple imputation,p-value,Posterior predictive check,Prior predictive check,u-value,Vérification de modéle,Vérification prédictive a posteriori,Vérification prédictive antérieur},
  pages = {369-382},
  file = {/home/osvaldo/Zotero/storage/MSFE6XUX/j.1751-5823.2003.tb00203.html}
}


@incollection{DiaconisTheoriesDataAnalysis2011,
  title = {Theories of {{Data Analysis}}: {{From Magical Thinking Through Classical Statistics}}},
  copyright = {Copyright \textcopyright{} 1985, 2006 John Wiley \& Sons, Inc. All rights reserved.},
  isbn = {978-1-118-15070-2},
  shorttitle = {Theories of {{Data Analysis}}},
  abstract = {This chapter contains sections titled: Intuitive Statistics\textemdash{}Some Inferential Problems Multiplicity\textemdash{}A Pervasive Problem Some Remedies Theories for Data Analysis Uses for Mathematics In Defense of Controlled Magical Thinking},
  language = {en},
  booktitle = {Exploring {{Data Tables}}, {{Trends}}, and {{Shapes}}},
  publisher = {{John Wiley \& Sons, Ltd}},
  author = {Diaconis, Persi},
  year = {2011},
  keywords = {controlled magical thinking,data analysis,data structure,intuitive statistics,multiplicity},
  pages = {1-36},
  file = {/home/osvaldo/Zotero/storage/P4VQ9KRV/9781118150702.html},
  doi = {10.1002/9781118150702.ch1}
}


@article{bingham2018pyro,
  title = {Pyro: {{Deep Universal Probabilistic Programming}}},
  journal = {Journal of Machine Learning Research},
  author = {Bingham, Eli and Chen, Jonathan P. and Jankowiak, Martin and Obermeyer, Fritz and Pradhan, Neeraj and Karaletsos, Theofanis and Singh, Rohit and Szerlip, Paul and Horsfall, Paul and Goodman, Noah D.},
  year = {2018},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1810.09538},
}

@article{emcee,
  title = {Emcee: {{The MCMC Hammer}}},
  volume = {125},
  doi = {10.1086/670067},
  journal = {PASP},
  author = {{Foreman-Mackey}, D. and Hogg, D. W. and Lang, D. and Goodman, J.},
  year = {2013},
  pages = {306-312},
  eprint = {1202.3665}
}


@article{SalvatierProbabilisticProgrammingPython2016,
  title = {Probabilistic {{Programming}} in {{Python Using PyMC3}}},
  volume = {2},
  issn = {2376-5992},
  doi = {10.7717/peerj-cs.55},
  language = {en},
  journal = {PeerJ Computer Science},
  author = {Salvatier, John and Wiecki, Thomas V. and Fonnesbeck, Christopher},
  month = apr,
  year = {2016},
  pages = {e55}
}

@article{StanProbabilisticProgramming,
  title = {Stan: {{A Probabilistic Programming Language}} | {{Carpenter}} | {{Journal}} of {{Statistical Software}}},
  shorttitle = {Stan},
  doi = {10.18637/jss.v076.i01},
  language = {en-US},
  keywords = {Bayesian inference,algorithmic differentiation,probabilistic programming,Stan},
  file = {/home/osvaldo/Zotero/storage/D4FVXPDF/v076i01.html}
}

@article{TranEdwardlibraryprobabilistic2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1610.09787},
  primaryClass = {cs, stat},
  title = {Edward: {{A}} Library for Probabilistic Modeling, Inference, and Criticism},
  shorttitle = {Edward},
  abstract = {Probabilistic modeling is a powerful approach for analyzing empirical information. We describe Edward, a library for probabilistic modeling. Edward's design reflects an iterative process pioneered by George Box: build a model of a phenomenon, make inferences about the model given data, and criticize the model's fit to the data. Edward supports a broad class of probabilistic models, efficient algorithms for inference, and many techniques for model criticism. The library builds on top of TensorFlow to support distributed training and hardware such as GPUs. Edward enables the development of complex probabilistic models and their algorithms at a massive scale.},
  journal = {arXiv:1610.09787 [cs, stat]},
  author = {Tran, Dustin and Kucukelbir, Alp and Dieng, Adji B. and Rudolph, Maja and Liang, Dawen and Blei, David M.},
  month = oct,
  year = {2016},
  keywords = {Statistics - Computation,Computer Science - Artificial Intelligence,Statistics - Machine Learning,Statistics - Applications,Computer Science - Programming Languages},
  file = {/home/osvaldo/Zotero/storage/PD4M3ZKL/Tran et al. - 2016 - Edward A library for probabilistic modeling, infe.pdf;/home/osvaldo/Zotero/storage/LX37VDW6/1610.html}
}

@article{TranDeepProbabilisticProgramming2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1701.03757},
  primaryClass = {cs, stat},
  title = {Deep {{Probabilistic Programming}}},
  abstract = {We propose Edward, a Turing-complete probabilistic programming language. Edward defines two compositional representations---random variables and inference. By treating inference as a first class citizen, on a par with modeling, we show that probabilistic programming can be as flexible and computationally efficient as traditional deep learning. For flexibility, Edward makes it easy to fit the same model using a variety of composable inference methods, ranging from point estimation to variational inference to MCMC. In addition, Edward can reuse the modeling representation as part of inference, facilitating the design of rich variational models and generative adversarial networks. For efficiency, Edward is integrated into TensorFlow, providing significant speedups over existing probabilistic systems. For example, we show on a benchmark logistic regression task that Edward is at least 35x faster than Stan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it is as fast as handwritten TensorFlow.},
  journal = {arXiv:1701.03757 [cs, stat]},
  author = {Tran, Dustin and Hoffman, Matthew D. and Saurous, Rif A. and Brevdo, Eugene and Murphy, Kevin and Blei, David M.},
  month = jan,
  year = {2017},
  keywords = {Statistics - Computation,Computer Science - Artificial Intelligence,Statistics - Machine Learning,Computer Science - Machine Learning,Computer Science - Programming Languages},
  file = {/home/osvaldo/Zotero/storage/7EJVRYHP/Tran et al. - 2017 - Deep Probabilistic Programming.pdf;/home/osvaldo/Zotero/storage/FBQ79439/1701.html},
  annote = {Comment: Appears in International Conference on Learning Representations, 2017. A companion webpage for this paper is available at http://edwardlib.org/iclr2017}
}
